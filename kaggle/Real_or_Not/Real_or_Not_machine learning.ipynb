{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What files do I need?\n",
    "You'll need train.csv, test.csv and sample_submission.csv.\n",
    "\n",
    "## What should I expect the data format to be?\n",
    "- Each sample in the train and test set has the following information:\n",
    "- The text of a tweet\n",
    "- A keyword from that tweet (although this may be blank!)\n",
    "- The location the tweet was sent from (may also be blank)\n",
    "\n",
    "## What am I predicting?\n",
    "You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
    "\n",
    "## Files\n",
    "- train.csv - the training set\n",
    "- test.csv - the test set\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "## Columns\n",
    "- id - a unique identifier for each tweet\n",
    "- text - the text of the tweet\n",
    "- location - the location the tweet was sent from (may be blank)\n",
    "- keyword - a particular keyword from the tweet (may be blank)\n",
    "- target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\home\\WorkSpace\\kaggle\\Real_or_Not\\train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv(r'C:\\Users\\home\\WorkSpace\\kaggle\\Real_or_Not\\test.csv', dtype={'id': np.int16})\n",
    "df_sub = pd.read_csv(r'C:\\Users\\home\\WorkSpace\\kaggle\\Real_or_Not\\sample_submission.csv', dtype={'id': np.int16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data exploring&cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int16 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int8  \n",
      "dtypes: int16(1), int8(1), object(3)\n",
      "memory usage: 200.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int16 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int16(1), object(3)\n",
      "memory usage: 83.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 4)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df_train = df_train.groupby('target').target.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['disaster', 'non_disaster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD5CAYAAADLL+UrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQH0lEQVR4nO3df7BcZX3H8feHxEKs8msIDCbYME6qglJsbtNUi0XLlNQfDU5lJrZKpsVGGfyBo+OAnVFrxymOju1QBYtWCUWLabVDpFLFVEQtJd4gEgEpqVhIYSBMRaFabOK3f+zDuL3cm3vza+8lz/s1c2bP+Z7nOXv2ztnPPfvsnt1UFZKkPhw02zsgSRodQ1+SOmLoS1JHDH1J6oihL0kdmT/bOzCdo446qpYsWTLbuyFJTyibN29+sKoWTqzP+dBfsmQJ4+Pjs70bkvSEkuQ/Jqs7vCNJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2Z81fk7pVktvdAc5U/HqROeaYvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI7MOPSTzEvyzSRXt+Ujk1yb5M52e8RQ2wuSbE1yR5LTh+rLkmxp6y5K/EY0SRql3TnTfzNw+9Dy+cDGqloKbGzLJDkBWA2cCKwELk4yr/W5BFgLLG3Tyr3ae0nSbplR6CdZDLwU+NhQeRWwrs2vA84Yql9ZVY9W1V3AVmB5kmOBQ6vqhqoq4PKhPpKkEZjpmf5fAG8HfjpUO6aq7gNot0e3+iLgnqF221ptUZufWH+cJGuTjCcZ3759+wx3UZI0nWlDP8nLgAeqavMMtznZOH3tov74YtWlVTVWVWMLFy6c4d1KkqYzk1/OegHwO0leAhwCHJrkCuD+JMdW1X1t6OaB1n4bcNxQ/8XAva2+eJK6JGlEpj3Tr6oLqmpxVS1h8AbtP1fVq4ENwJrWbA1wVZvfAKxOcnCS4xm8YbupDQE9nGRF+9TOWUN9JEkjsDe/kXshsD7J2cDdwJkAVXVrkvXAbcAO4Nyq2tn6nANcBiwArmmTJGlEUnP8B6LHxsZqfHx8zzp7GYCmMsePe2lvJdlcVWMT616RK0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkfmzvQNSz5LZ3gPNVVX7Z7ue6UtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkemDf0khyTZlORbSW5N8ietfmSSa5Pc2W6PGOpzQZKtSe5IcvpQfVmSLW3dRYnXI0rSKM3kTP9R4MVV9UvAycDKJCuA84GNVbUU2NiWSXICsBo4EVgJXJxkXtvWJcBaYGmbVu7DxyJJmsa0oV8Dj7TFJ7WpgFXAulZfB5zR5lcBV1bVo1V1F7AVWJ7kWODQqrqhqgq4fKiPJGkEZjSmn2RekpuBB4Brq+pG4Jiqug+g3R7dmi8C7hnqvq3VFrX5ifXJ7m9tkvEk49u3b9+dxyNJ2oUZhX5V7ayqk4HFDM7an7OL5pON09cu6pPd36VVNVZVYwsXLpzJLkqSZmC3Pr1TVQ8B1zEYi7+/DdnQbh9ozbYBxw11Wwzc2+qLJ6lLkkZkJp/eWZjk8Da/ADgN+A6wAVjTmq0BrmrzG4DVSQ5OcjyDN2w3tSGgh5OsaJ/aOWuojyRpBGbyIyrHAuvaJ3AOAtZX1dVJbgDWJzkbuBs4E6Cqbk2yHrgN2AGcW1U727bOAS4DFgDXtEmSNCKp/fXzLPvI2NhYjY+P71lnLwPQVObIce8hqqns7SGaZHNVjU2se0WuJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerItKGf5LgkX05ye5Jbk7y51Y9Mcm2SO9vtEUN9LkiyNckdSU4fqi9LsqWtuyhJ9s/DkiRNZiZn+juAt1bVs4EVwLlJTgDOBzZW1VJgY1umrVsNnAisBC5OMq9t6xJgLbC0TSv34WORJE1j2tCvqvuq6qY2/zBwO7AIWAWsa83WAWe0+VXAlVX1aFXdBWwFlic5Fji0qm6oqgIuH+ojSRqB3RrTT7IEeB5wI3BMVd0Hg38MwNGt2SLgnqFu21ptUZufWJckjciMQz/JU4DPAOdV1Q931XSSWu2iPtl9rU0ynmR8+/btM91FSdI0ZhT6SZ7EIPA/WVWfbeX725AN7faBVt8GHDfUfTFwb6svnqT+OFV1aVWNVdXYwoULZ/pYJEnTmMmndwL8NXB7VX1waNUGYE2bXwNcNVRfneTgJMczeMN2UxsCejjJirbNs4b6SJJGYP4M2rwAeA2wJcnNrfYO4EJgfZKzgbuBMwGq6tYk64HbGHzy59yq2tn6nQNcBiwArmmTJGlEMvggzdw1NjZW4+Pje9bZywA0lTly3HuIaip7e4gm2VxVYxPrXpErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR6YN/SQfT/JAkm8P1Y5Mcm2SO9vtEUPrLkiyNckdSU4fqi9LsqWtuyhJ9v3DkSTtykzO9C8DVk6onQ9srKqlwMa2TJITgNXAia3PxUnmtT6XAGuBpW2auE1J0n42behX1fXAf00orwLWtfl1wBlD9Sur6tGqugvYCixPcixwaFXdUFUFXD7UR5I0Ins6pn9MVd0H0G6PbvVFwD1D7ba12qI2P7E+qSRrk4wnGd++ffse7qIkaaJ9/UbuZOP0tYv6pKrq0qoaq6qxhQsX7rOdk6Te7Wno39+GbGi3D7T6NuC4oXaLgXtbffEkdUnSCO1p6G8A1rT5NcBVQ/XVSQ5OcjyDN2w3tSGgh5OsaJ/aOWuojyRpROZP1yDJ3wKnAkcl2Qa8C7gQWJ/kbOBu4EyAqro1yXrgNmAHcG5V7WybOofBJ4EWANe0SZI0Qhl8mGbuGhsbq/Hx8T3r7KUAmsocOe49RDWVvT1Ek2yuqrGJda/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSMjD/0kK5PckWRrkvNHff+S1LORhn6SecCHgd8GTgBeleSEUe6DJPVs1Gf6y4GtVfXdqvoJcCWwasT7IEndmj/i+1sE3DO0vA341YmNkqwF1rbFR5LcMYJ968FRwIOzvRNzQjLbe6DJeYw2++AQ/YXJiqMO/ckeRj2uUHUpcOn+352+JBmvqrHZ3g9pKh6j+9+oh3e2AccNLS8G7h3xPkhSt0Yd+t8AliY5PsnPAauBDSPeB0nq1kiHd6pqR5I3AF8A5gEfr6pbR7kPnXPITHOdx+h+lqrHDalLkg5QXpErSR0x9CWpI6P+yKakA1iSdwOPAIcC11fVl/bBNpcAz6+qT+3ttuSZ/gEtyXVJxtr855Mcvo+2e2qS5++LbenAVFXv3BeB3ywBfm93OrSvfNEkDP1OVNVLquqhfbS5U4HdCv0kvqo8QCX54/Ylil8CntlqlyV5ZZu/MMltSW5J8oFWe3mSG5N8M8mXkhzT6r+R5OY2fTPJU4ELgVNa7S1J5iV5f5JvtG2+rvU9NcmXk3wK2DIbf4snAp+II9Zeql4DfI1BcP4ng+8feibwEeDJwL8Df1hV309yHXAj8CLgcODsqvrqFNteAHyCwZfZ3Q4sGFr3PWAM+DGwnsGFcfOAP62qTyd5J/Dy1udfgNdVVSV5E/B6YAdwG3B+W96Z5NXAG4HvtH1/eru786rq6+2l/tMYnKk9yG6erWnuS7KMwfU2z2OQJzcBm4fWHwm8AnhWO54ee7X5NWBFq70WeDvwVuBtwLnt+HkK8D8Mjrm3VdXL2jbXAj+oql9JcjDw9SRfbNtdDjynqu7av4/8icvQnx1LgVdV1R8lWQ/8LoOD/o1V9ZUk7wHeBZzX2s+vquVJXtLqp02x3XOAH1XVSUlOYvAEnGglcG9VvRQgyWGt/qGqek+r/Q3wMuBzDJ5wx1fVo0kOr6qHknwEeKSqHjtr+xTw51X1tSRPZ3AdxrPbdpcBv15VP96Dv5PmvlOAf6iqHwEkmXix5Q8ZBPfHkvwjcHWrLwY+neRY4OeAx0L668AHk3wS+GxVbcvjv4Tmt4CTHnslARzG4Dn1E2CTgb9rDu/Mjruq6uY2vxl4BnB4VX2l1dYBLxxq/9mhtkt2sd0XAlcAVNUtwC2TtNkCnJbkfUlOqaoftPqL2svtLcCLgRNb/Rbgk+2sfscU93sa8KEkNzO4wvrQ9rIcYIOBf8Cb8mKfqtrB4Oz7M8AZwD+1VX/J4ETjucDrgENa+wuB1zJ4xfmvSZ41yWbD4ATp5DYdX1WPnen/9754QAcyQ392PDo0v5PBsM1M2u9k+ldnu7zarqr+jcHZ9xbgz5K8M8khwMXAK9uT8KO0JyHwUga/gbAM2DzF2PxBwK8NPQkXVdXDbZ1PwgPb9cArkixo/+hfPryyDdEcVlWfZ/DK9eS26jAGQ5sAa4baP6OqtlTV+4Bx4FnAw8BTf7ZVvgCck+RJrc8vJvn5ff/QDkyG/tzwA+D7SU5py68BvrKL9lO5Hvh9gCTPAU6a2CDJ0xgMAV0BfAD4ZX4W8A+2J+ljb8AdBBxXVV9mMPx0OPAUHv8k/CLwhqH7OBl1oapuAj4N3MzgbH7i+01PBa5OcguDY/otrf5u4O+SfJX//1XK5yX5dpJvMXj/6RoGrzZ3JPlWkrcAH2Pw/tJNSb4N/BUOVc+Yf6i5Yw3wkSRPBr4L/MEebOMS4BPtCXYzsGmSNs8F3p/kp8D/Aue0cfqPMjj7/x6DL8aDwRu9V7Rx/zAYt38oyeeAv0+yisEbuW8CPtzudz6Dfz6v34P91xNQVb0XeO8umiyfpM9VwFWT1N84xTZ+c8LyO9o07Lo2aRf87h1J6ojDO5LUEYd3noCSnA68b0L5rqp6xWzsj6QnDod3JKkjDu9IUkcMfUnqiKEvSR0x9CWpI/8Hc10jlAE25IUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(label[1],target_df_train[0], color = 'r')\n",
    "plt.bar(label[0],target_df_train[1], color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제 를 풀려면 텍스트에 대한 분류와 binary classification이 필요할 것 같다. 이 부분에서 공부를 해야할 것 같다. \n",
    "텍스트를 통한 KEYWORD분류도 가능할 것 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    our deeds are the reason of this earthquake ma...\n",
       "1                forest fire near la ronge sask canada\n",
       "2    all residents asked to shelter in place are be...\n",
       "3     people receive wildfires evacuation orders in...\n",
       "4    just got sent this photo from ruby alaska as s...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying a first round of text cleaning techniques\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "df_train['text'] = df_train['text'].apply(lambda x: clean_text(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "df_train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         deeds reason earthquake may allah forgive us\n",
       "1                forest fire near la ronge sask canada\n",
       "2    residents asked shelter place notified officer...\n",
       "3    people receive wildfires evacuation orders cal...\n",
       "4    got sent photo ruby alaska smoke wildfires pou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x:\" \".join(term for term in x.split() if term not in stop_words))\n",
    "df_test['text'] = df_test['text'].apply(lambda x:\" \".join(term for term in x.split() if term not in stop_words))\n",
    "\n",
    "df_train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "tokens = df_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deeds, reason, earthquake, may, allah, forgiv...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [residents, asked, shelter, place, notified, o...\n",
       "3       [people, receive, wildfires, evacuation, order...\n",
       "4       [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [ariaahrary, thetawniest, control, wild, fires...\n",
       "7610                                    [volcano, hawaii]\n",
       "7611    [police, investigating, ebike, collided, car, ...\n",
       "7612    [latest, homes, razed, northern, california, w...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer,Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-0ad384425df1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "token.lower\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x:\" \".join(stemmer.stem(token) for token in tokens))\n",
    "df_test['text'] = df_test['text'].apply(lambda x:\" \".join(stemmer.stem(token) for token in tokens))\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x:\" \".join(lemmatizer.lemmatize(token) for token in tokens))\n",
    "df_test['text'] = df_test['text'].apply(lambda x:\" \".join(lemmatizer.lemmatize(token) for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         deeds reason earthquake may allah forgive us\n",
       "1                forest fire near la ronge sask canada\n",
       "2    residents asked shelter place notified officer...\n",
       "3    people receive wildfires evacuation orders cal...\n",
       "4    got sent photo ruby alaska smoke wildfires pou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x : combine_text(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x : combine_text(x))\n",
    "df_train['text']\n",
    "df_train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(df_train['text'])\n",
    "test_vectors = count_vectorizer.transform(df_test['text'])\n",
    "\n",
    "## Keeping only non-zero elements to preserve space \n",
    "print(train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6029)\t0.4127241670499736\n",
      "  (0, 10227)\t0.23716098275777425\n",
      "  (0, 3754)\t0.42660875382968927\n",
      "  (0, 217)\t0.37358571513536404\n",
      "  (0, 6028)\t0.2646487348216956\n",
      "  (0, 2897)\t0.2959083674601754\n",
      "  (0, 7879)\t0.33504681937478503\n",
      "  (0, 2414)\t0.42660875382968927\n",
      "  (1, 3589)\t0.4781907991017602\n",
      "  (1, 3744)\t0.42162281679203917\n",
      "  (1, 1417)\t0.41675720429854407\n",
      "  (1, 5351)\t0.3757141854237119\n",
      "  (1, 6530)\t0.33265029752231373\n",
      "  (1, 3570)\t0.2511337989166678\n",
      "  (1, 3743)\t0.32403030437201963\n",
      "  (2, 3253)\t0.2580872844705925\n",
      "  (2, 6970)\t0.2684495282245868\n",
      "  (2, 3168)\t0.2177728140293216\n",
      "  (2, 6832)\t0.2830542905170327\n",
      "  (2, 7328)\t0.48119880358519396\n",
      "  (2, 8655)\t0.586833068542054\n",
      "  (2, 544)\t0.2788117719785811\n",
      "  (2, 8061)\t0.2830542905170327\n",
      "  (3, 3176)\t0.4706106619990253\n",
      "  (3, 1369)\t0.2764742850446076\n",
      "  :\t:\n",
      "  (7611, 7477)\t0.20005204986869024\n",
      "  (7611, 2924)\t0.4001040997373805\n",
      "  (7611, 1876)\t0.1397909095273928\n",
      "  (7611, 9690)\t0.184364364714822\n",
      "  (7611, 4977)\t0.16216570194521843\n",
      "  (7611, 8568)\t0.16518785727442115\n",
      "  (7611, 5687)\t0.13777780149638608\n",
      "  (7611, 8136)\t0.184364364714822\n",
      "  (7611, 7424)\t0.11528015755494936\n",
      "  (7611, 1468)\t0.12410322437352454\n",
      "  (7612, 10732)\t0.30021294801237225\n",
      "  (7612, 7820)\t0.27283778096615524\n",
      "  (7612, 4594)\t0.2714300194284437\n",
      "  (7612, 5436)\t0.27283778096615524\n",
      "  (7612, 7818)\t0.2674722653243108\n",
      "  (7612, 10)\t0.2926420474941421\n",
      "  (7612, 9)\t0.27900091994667386\n",
      "  (7612, 4590)\t0.24264709084451508\n",
      "  (7612, 1381)\t0.26502891424750974\n",
      "  (7612, 10731)\t0.23450342278570369\n",
      "  (7612, 6712)\t0.25745801372927957\n",
      "  (7612, 6711)\t0.23932331834244627\n",
      "  (7612, 5435)\t0.2433450062364687\n",
      "  (7612, 6622)\t0.19725856595871938\n",
      "  (7612, 1369)\t0.21563144975266887\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tfidf = tfidf.fit_transform(df_train['text'])\n",
    "test_tfidf = tfidf.transform(df_test['text'])\n",
    "print(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X = df_train.text\n",
    "y = df_train.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score function\n",
    "\n",
    "def acc_summary(pipeline, X_train, y_train, X_test, y_test):\n",
    "    sentiment_fit = pipeline.fit(X_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "   \n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some model and their performance\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"Bernouli\", \"PassiveAggressiveClassifier\",\n",
    "     \"Naive Bayes\", \"SVC\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    PassiveAggressiveClassifier(max_iter=50),\n",
    "    SVC(kernel=\"linear\")\n",
    "]\n",
    "    \n",
    "zipped_clf = zip(names, classifiers)\n",
    "tvec = TfidfVectorizer()\n",
    "    \n",
    "def compare_clf(classifier=zipped_clf, vectorizer=tvec, n_features=10000, ngram_range=(1, 1)):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n, c in classifier:\n",
    "        checker_pipeline = Pipeline([\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", c)\n",
    "        ])\n",
    "        clf_acc = acc_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n",
    "        print(\"Model result for {}\".format(n))\n",
    "        print(c)\n",
    "        result.append((n, clf_acc))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "accuracy score: 67.29%\n",
      "------------------------------\n",
      "Model result for K Nearest Neighbors\n",
      "KNeighborsClassifier(n_neighbors=3)\n",
      "------------------------------\n",
      "accuracy score: 72.15%\n",
      "------------------------------\n",
      "Model result for Decision Tree\n",
      "DecisionTreeClassifier(random_state=0)\n",
      "------------------------------\n",
      "accuracy score: 77.58%\n",
      "------------------------------\n",
      "Model result for Random Forest\n",
      "RandomForestClassifier()\n",
      "------------------------------\n",
      "accuracy score: 80.78%\n",
      "------------------------------\n",
      "Model result for Logistic Regression\n",
      "LogisticRegression()\n",
      "------------------------------\n",
      "accuracy score: 80.34%\n",
      "------------------------------\n",
      "Model result for Bernouli\n",
      "MultinomialNB()\n",
      "------------------------------\n",
      "accuracy score: 80.52%\n",
      "------------------------------\n",
      "Model result for PassiveAggressiveClassifier\n",
      "BernoulliNB()\n",
      "------------------------------\n",
      "accuracy score: 74.17%\n",
      "------------------------------\n",
      "Model result for Naive Bayes\n",
      "PassiveAggressiveClassifier(max_iter=50)\n",
      "------------------------------\n",
      "accuracy score: 80.04%\n",
      "------------------------------\n",
      "Model result for SVC\n",
      "SVC(kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "trigram_result = compare_clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('K Nearest Neighbors', 0.6729422066549913),\n",
       " ('Decision Tree', 0.7215411558669002),\n",
       " ('Random Forest', 0.7758318739054291),\n",
       " ('Logistic Regression', 0.8077933450087565),\n",
       " ('Bernouli', 0.803415061295972),\n",
       " ('PassiveAggressiveClassifier', 0.8051663747810858),\n",
       " ('Naive Bayes', 0.7416812609457093),\n",
       " ('SVC', 0.8003502626970228)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "def prediction(pipeline, testtext):\n",
    "    sentiment_fit = pipeline.fit(X_train,y_train)\n",
    "    y_pred = sentiment_fit.predict(testtext)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df_test.id\n",
    "newFrame = pd.DataFrame({\"id\":index, \"target\":prediction})\n",
    "newFrame.to_csv(\"Real_or_Not_machine learning.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565217391304348"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(train_vectors, df_train[\"target\"])\n",
    "pred1 = clf.predict(train_vectors)\n",
    "accuracy_score(df_train[\"target\"],pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8802049126494155"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf = LogisticRegression(C=1.0)\n",
    "clf_tfidf.fit(train_tfidf, df_train[\"target\"])\n",
    "pred2 = clf.predict(train_tfidf)\n",
    "accuracy_score(df_train[\"target\"],pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129121239984237"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(train_vectors, df_train[\"target\"])\n",
    "pred3 = clf_NB.predict(train_vectors)\n",
    "accuracy_score(df_train[\"target\"],pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674635491921713"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB_TFIDF = MultinomialNB()\n",
    "clf_NB_TFIDF.fit(train_tfidf, df_train[\"target\"])\n",
    "pred4 = clf_NB.predict(train_tfidf)\n",
    "accuracy_score(df_train[\"target\"],pred4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8161040325758571"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf_xgb.fit(train_vectors, df_train[\"target\"])\n",
    "pred5 = clf_xgb.predict(train_vectors)\n",
    "accuracy_score(df_train[\"target\"],pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = df_train.values\n",
    "test_tfidf = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'deeds reason earthquake may allah forgive us'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-663acce7bd9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m clf_xgb_TFIDF = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n\u001b[0;32m      2\u001b[0m                         subsample=0.8, nthread=10, learning_rate=0.1)\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf_xgb_TFIDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_xgb_TFIDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[1;32m--> 726\u001b[1;33m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_dt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[1;34m(self, mat, missing, nthread)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;31m# we try to avoid data copies if possible (reshape returns a view when possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;31m# and we explicitly tell np.array to try and avoid copying)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'deeds reason earthquake may allah forgive us'"
     ]
    }
   ],
   "source": [
    "clf_xgb_TFIDF = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf_xgb_TFIDF.fit(train_tfidf, df_train[\"target\"])\n",
    "pred6 = clf_xgb_TFIDF.predict(train_vectors)\n",
    "accuracy_score(df_train[\"target\"],pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,model,test_vectors):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ablaze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-2e404fb077f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msubmission_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\home\\WorkSpace\\kaggle\\Real_or_Not\\sample_submission.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_vectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_tfidf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf_NB_TFIDF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-210-88f6a302c0f8>\u001b[0m in \u001b[0;36msubmission\u001b[1;34m(submission_file_path, model, test_vectors)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample_submission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m     76\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\Users\\home\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'ablaze'"
     ]
    }
   ],
   "source": [
    "submission_file_path = r\"C:\\Users\\home\\WorkSpace\\kaggle\\Real_or_Not\\sample_submission.csv\"\n",
    "test_vectors=test_tfidf\n",
    "submission(submission_file_path,clf_NB_TFIDF,test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
